{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.0\n",
      "  Downloading pyspark-3.0.0.tar.gz (204.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 204.7 MB 39 kB/s s eta 0:00:01        | 147.0 MB 84.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.9\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 30.1 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.0.0-py2.py3-none-any.whl size=205044182 sha256=b8fef102b516c5442a6209b3d83c745881135b1154193abef7b1b7128f8c3522\n",
      "  Stored in directory: /home/spark/shared/.cache/pip/wheels/4e/c5/36/aef1bb711963a619063119cc032176106827a129c0be20e301\n",
      "Successfully built pyspark\n",
      "\u001b[31mERROR: sparktspy-nojars 2.0.5.0 has requirement pyspark==3.0.1, but you'll have pyspark 3.0.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9 pyspark-3.0.0\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/py4j already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/pyspark already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/share already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.0\n",
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'HMP_Dataset'...\n",
      "remote: Enumerating objects: 865, done.\u001b[K\n",
      "remote: Total 865 (delta 0), reused 0 (delta 0), pack-reused 865\u001b[K\n",
      "Receiving objects: 100% (865/865), 1010.96 KiB | 0 bytes/s, done.\n",
      "Checking out files: 100% (848/848), done.\n"
     ]
    }
   ],
   "source": [
    "# Import data from github\n",
    "!git clone https://github.com/wchill/HMP_Dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brush_teeth\tDrink_glass  Getup_bed\t  Pour_water\t Use_telephone\n",
      "Climb_stairs\tEat_meat     impdata.py   README.txt\t Walk\n",
      "Comb_hair\tEat_soup     Liedown_bed  Sitdown_chair\n",
      "Descend_stairs\tfinal.py     MANUAL.txt   Standup_chair\n"
     ]
    }
   ],
   "source": [
    "# View all datasets avaialble in the repository\n",
    "! ls HMP_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt\n",
      "Accelerometer-2011-04-11-13-29-54-brush_teeth-f1.txt\n",
      "Accelerometer-2011-05-30-08-35-11-brush_teeth-f1.txt\n",
      "Accelerometer-2011-05-30-09-36-50-brush_teeth-f1.txt\n",
      "Accelerometer-2011-05-30-10-34-16-brush_teeth-m1.txt\n",
      "Accelerometer-2011-05-30-21-10-57-brush_teeth-f1.txt\n",
      "Accelerometer-2011-05-30-21-55-04-brush_teeth-m2.txt\n",
      "Accelerometer-2011-05-31-15-16-47-brush_teeth-f1.txt\n",
      "Accelerometer-2011-06-02-10-42-22-brush_teeth-f1.txt\n",
      "Accelerometer-2011-06-02-10-45-50-brush_teeth-f1.txt\n",
      "Accelerometer-2011-06-06-10-45-27-brush_teeth-f1.txt\n",
      "Accelerometer-2011-06-06-10-48-05-brush_teeth-f1.txt\n"
     ]
    }
   ],
   "source": [
    "# Viewing all datasets avaialble in Brush teeth\n",
    "! ls HMP_Dataset/Brush_teeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries for creating schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Creating schema with 3 columns as per data\n",
    "schema = StructType({\n",
    "    StructField('x', IntegerType(), True),\n",
    "    StructField('y', IntegerType(), True),\n",
    "    StructField('z', IntegerType(), True),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import relevant library to help with datapreprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.idea',\n",
       " 'Brush_teeth',\n",
       " 'Climb_stairs',\n",
       " 'Comb_hair',\n",
       " 'Descend_stairs',\n",
       " 'Drink_glass',\n",
       " 'Eat_meat',\n",
       " 'Eat_soup',\n",
       " 'Getup_bed',\n",
       " 'Liedown_bed',\n",
       " 'MANUAL.txt',\n",
       " 'Pour_water',\n",
       " 'README.txt',\n",
       " 'Sitdown_chair',\n",
       " 'Standup_chair',\n",
       " 'Use_telephone',\n",
       " 'Walk',\n",
       " 'final.py',\n",
       " 'impdata.py']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of folders/files in folder HMP_Dataset\n",
    "file_list = os.listdir('HMP_Dataset')\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brush_teeth',\n",
       " 'Climb_stairs',\n",
       " 'Comb_hair',\n",
       " 'Descend_stairs',\n",
       " 'Drink_glass',\n",
       " 'Eat_meat',\n",
       " 'Eat_soup',\n",
       " 'Getup_bed',\n",
       " 'Liedown_bed',\n",
       " 'Pour_water',\n",
       " 'Sitdown_chair',\n",
       " 'Standup_chair',\n",
       " 'Use_telephone',\n",
       " 'Walk']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter list for folders containing data\n",
    "file_list_filtered = [s for s in file_list if '.' not in s]\n",
    "file_list_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data as a schema\n",
    "df = None\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "for category in file_list_filtered:\n",
    "    data_files = os.listdir('HMP_Dataset/'+category)\n",
    "    \n",
    "    for data_file in data_files:\n",
    "        #print(data_file)\n",
    "        temp_df = spark.read.option('header', 'false').option('delimiter', ' ').csv('HMP_Dataset/'+category+'/'+data_file, schema = schema)\n",
    "        temp_df = temp_df.withColumn('class', lit(category))\n",
    "        temp_df = temp_df.withColumn('source', lit(data_file))\n",
    "        \n",
    "        if df is None:\n",
    "            df = temp_df\n",
    "        else:\n",
    "            df = df.union(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------------------+\n",
      "|  z|  x|  y|      class|              source|\n",
      "+---+---+---+-----------+--------------------+\n",
      "| 22| 49| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 22| 49| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 22| 52| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 22| 52| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 21| 52| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 22| 51| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 20| 50| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 22| 52| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 22| 50| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 22| 51| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 21| 51| 33|Brush_teeth|Accelerometer-201...|\n",
      "| 20| 50| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 21| 49| 33|Brush_teeth|Accelerometer-201...|\n",
      "| 21| 49| 33|Brush_teeth|Accelerometer-201...|\n",
      "| 20| 51| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 18| 49| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 19| 48| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 16| 53| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 18| 52| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 18| 51| 32|Brush_teeth|Accelerometer-201...|\n",
      "+---+---+---+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show schema\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446529"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read/Save into PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('HMP.parquet')\n",
    "\n",
    "#df = spark.read.parquet('HMP.parquet')\n",
    "#df.createOrReplaceTempView('HMP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(False, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------------------+\n",
      "|  z|  x|  y|      class|              source|\n",
      "+---+---+---+-----------+--------------------+\n",
      "| 24| 49| 39|Brush_teeth|Accelerometer-201...|\n",
      "| 38| 54| 42|Brush_teeth|Accelerometer-201...|\n",
      "| 22| 49| 45|Brush_teeth|Accelerometer-201...|\n",
      "| 31| 52| 39|Brush_teeth|Accelerometer-201...|\n",
      "| 34| 54| 41|Brush_teeth|Accelerometer-201...|\n",
      "| 37| 53| 30|Brush_teeth|Accelerometer-201...|\n",
      "| 25| 39| 49|Brush_teeth|Accelerometer-201...|\n",
      "| 32| 49| 12|Brush_teeth|Accelerometer-201...|\n",
      "| 26| 31| 41|Brush_teeth|Accelerometer-201...|\n",
      "| 54| 47| 43|Brush_teeth|Accelerometer-201...|\n",
      "|  2| 33| 41|Brush_teeth|Accelerometer-201...|\n",
      "|  0| 41| 43|Brush_teeth|Accelerometer-201...|\n",
      "| 13| 39| 38|Brush_teeth|Accelerometer-201...|\n",
      "| 12| 39| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 35| 52| 45|Brush_teeth|Accelerometer-201...|\n",
      "| 23| 49| 40|Brush_teeth|Accelerometer-201...|\n",
      "| 31| 55| 37|Brush_teeth|Accelerometer-201...|\n",
      "| 30| 48| 41|Brush_teeth|Accelerometer-201...|\n",
      "| 20| 49| 38|Brush_teeth|Accelerometer-201...|\n",
      "| 44| 55| 34|Brush_teeth|Accelerometer-201...|\n",
      "+---+---+---+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------------------+----------+\n",
      "|  z|  x|  y|      class|              source|classIndex|\n",
      "+---+---+---+-----------+--------------------+----------+\n",
      "| 24| 49| 39|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 38| 54| 42|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 22| 49| 45|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 31| 52| 39|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 34| 54| 41|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 37| 53| 30|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 25| 39| 49|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 32| 49| 12|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 26| 31| 41|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 54| 47| 43|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "|  2| 33| 41|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "|  0| 41| 43|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 13| 39| 38|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 12| 39| 34|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 35| 52| 45|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 23| 49| 40|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 31| 55| 37|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 30| 48| 41|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 20| 49| 38|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 44| 55| 34|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "+---+---+---+-----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index the class name aka automatic mapping\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol = 'class', outputCol = 'classIndex')\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------------------+----------+--------------+\n",
      "|  z|  x|  y|      class|              source|classIndex|categoryVector|\n",
      "+---+---+---+-----------+--------------------+----------+--------------+\n",
      "| 24| 49| 39|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 38| 54| 42|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 22| 49| 45|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 31| 52| 39|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 34| 54| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 37| 53| 30|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 25| 39| 49|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 32| 49| 12|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 26| 31| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 54| 47| 43|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "|  2| 33| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "|  0| 41| 43|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 13| 39| 38|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 12| 39| 34|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 35| 52| 45|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 23| 49| 40|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 31| 55| 37|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 30| 48| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 20| 49| 38|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "| 44| 55| 34|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|\n",
      "+---+---+---+-----------+--------------------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encode the class index\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCol = 'classIndex', outputCol = 'categoryVector')\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------------------+----------+--------------+----------------+\n",
      "|  z|  x|  y|      class|              source|classIndex|categoryVector|        features|\n",
      "+---+---+---+-----------+--------------------+----------+--------------+----------------+\n",
      "| 24| 49| 39|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[49.0,39.0,24.0]|\n",
      "| 38| 54| 42|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[54.0,42.0,38.0]|\n",
      "| 22| 49| 45|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[49.0,45.0,22.0]|\n",
      "| 31| 52| 39|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[52.0,39.0,31.0]|\n",
      "| 34| 54| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[54.0,41.0,34.0]|\n",
      "| 37| 53| 30|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[53.0,30.0,37.0]|\n",
      "| 25| 39| 49|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[39.0,49.0,25.0]|\n",
      "| 32| 49| 12|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[49.0,12.0,32.0]|\n",
      "| 26| 31| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[31.0,41.0,26.0]|\n",
      "| 54| 47| 43|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[47.0,43.0,54.0]|\n",
      "|  2| 33| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])| [33.0,41.0,2.0]|\n",
      "|  0| 41| 43|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])| [41.0,43.0,0.0]|\n",
      "| 13| 39| 38|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[39.0,38.0,13.0]|\n",
      "| 12| 39| 34|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[39.0,34.0,12.0]|\n",
      "| 35| 52| 45|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[52.0,45.0,35.0]|\n",
      "| 23| 49| 40|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[49.0,40.0,23.0]|\n",
      "| 31| 55| 37|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[55.0,37.0,31.0]|\n",
      "| 30| 48| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[48.0,41.0,30.0]|\n",
      "| 20| 49| 38|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[49.0,38.0,20.0]|\n",
      "| 44| 55| 34|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[55.0,34.0,44.0]|\n",
      "+---+---+---+-----------+--------------------+----------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Turn x,y,z into a single vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['x','y','z'], outputCol = 'features')\n",
    "vectorAssembled = vectorAssembler.transform(encoded)\n",
    "\n",
    "vectorAssembled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
      "|  z|  x|  y|      class|              source|classIndex|categoryVector|        features|  normalizedFeatures|\n",
      "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
      "| 24| 49| 39|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[49.0,39.0,24.0]|[0.73061124874900...|\n",
      "| 38| 54| 42|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[54.0,42.0,38.0]|[0.69004302254196...|\n",
      "| 22| 49| 45|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[49.0,45.0,22.0]|[0.69928680572414...|\n",
      "| 31| 52| 39|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[52.0,39.0,31.0]|[0.72208294496276...|\n",
      "| 34| 54| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[54.0,41.0,34.0]|[0.71194521064855...|\n",
      "| 37| 53| 30|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[53.0,30.0,37.0]|[0.74375435432843...|\n",
      "| 25| 39| 49|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[39.0,49.0,25.0]|[0.57836516827081...|\n",
      "| 32| 49| 12|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[49.0,12.0,32.0]|[0.82020574338016...|\n",
      "| 26| 31| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[31.0,41.0,26.0]|[0.53817497955558...|\n",
      "| 54| 47| 43|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[47.0,43.0,54.0]|[0.56280362547594...|\n",
      "|  2| 33| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])| [33.0,41.0,2.0]|[0.62655718055472...|\n",
      "|  0| 41| 43|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])| [41.0,43.0,0.0]|[0.69007533357283...|\n",
      "| 13| 39| 38|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[39.0,38.0,13.0]|[0.69665075314468...|\n",
      "| 12| 39| 34|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[39.0,34.0,12.0]|[0.73428230736716...|\n",
      "| 35| 52| 45|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[52.0,45.0,35.0]|[0.67390538773311...|\n",
      "| 23| 49| 40|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[49.0,40.0,23.0]|[0.72802614998033...|\n",
      "| 31| 55| 37|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[55.0,37.0,31.0]|[0.75159338988322...|\n",
      "| 30| 48| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[48.0,41.0,30.0]|[0.68676626427047...|\n",
      "| 20| 49| 38|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[49.0,38.0,20.0]|[0.75206821232870...|\n",
      "| 44| 55| 34|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[55.0,34.0,44.0]|[0.70322361963463...|\n",
      "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "normaliser = Normalizer(inputCol = 'features', outputCol = 'normalizedFeatures')\n",
    "normalised = normaliser.transform(vectorAssembled)\n",
    "\n",
    "normalised.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
      "|  z|  x|  y|      class|              source|classIndex|categoryVector|        features|  normalizedFeatures|\n",
      "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
      "| 24| 49| 39|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[49.0,39.0,24.0]|[0.73061124874900...|\n",
      "| 38| 54| 42|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[54.0,42.0,38.0]|[0.69004302254196...|\n",
      "| 22| 49| 45|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[49.0,45.0,22.0]|[0.69928680572414...|\n",
      "| 31| 52| 39|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[52.0,39.0,31.0]|[0.72208294496276...|\n",
      "| 34| 54| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[54.0,41.0,34.0]|[0.71194521064855...|\n",
      "| 37| 53| 30|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[53.0,30.0,37.0]|[0.74375435432843...|\n",
      "| 25| 39| 49|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[39.0,49.0,25.0]|[0.57836516827081...|\n",
      "| 32| 49| 12|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[49.0,12.0,32.0]|[0.82020574338016...|\n",
      "| 26| 31| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[31.0,41.0,26.0]|[0.53817497955558...|\n",
      "| 54| 47| 43|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[47.0,43.0,54.0]|[0.56280362547594...|\n",
      "|  2| 33| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])| [33.0,41.0,2.0]|[0.62655718055472...|\n",
      "|  0| 41| 43|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])| [41.0,43.0,0.0]|[0.69007533357283...|\n",
      "| 13| 39| 38|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[39.0,38.0,13.0]|[0.69665075314468...|\n",
      "| 12| 39| 34|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[39.0,34.0,12.0]|[0.73428230736716...|\n",
      "| 35| 52| 45|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[52.0,45.0,35.0]|[0.67390538773311...|\n",
      "| 23| 49| 40|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[49.0,40.0,23.0]|[0.72802614998033...|\n",
      "| 31| 55| 37|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[55.0,37.0,31.0]|[0.75159338988322...|\n",
      "| 30| 48| 41|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[48.0,41.0,30.0]|[0.68676626427047...|\n",
      "| 20| 49| 38|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[49.0,38.0,20.0]|[0.75206821232870...|\n",
      "| 44| 55| 34|Brush_teeth|Accelerometer-201...|       5.0|(13,[5],[1.0])|[55.0,34.0,44.0]|[0.70322361963463...|\n",
      "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a pipeline for preprocessing data\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages = [indexer,encoder,vectorAssembler,normaliser])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "prediction = model.transform(df)\n",
    "\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "\n",
    "d = 'HMP_Dataset/'\n",
    "\n",
    "# filter list for all folders containing data (folders that don't start with .)\n",
    "file_list_filtered = [s for s in os.listdir(d) if os.path.isdir(os.path.join(d,s)) & ~fnmatch.fnmatch(s, '.*')]\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#create pandas data frame for all the data\n",
    "\n",
    "df = None\n",
    "\n",
    "for category in file_list_filtered:\n",
    "    data_files = os.listdir('HMP_Dataset/'+category)\n",
    "    \n",
    "    #create a temporary pandas data frame for each data file\n",
    "    for data_file in data_files:\n",
    "        #print(data_file)\n",
    "        temp_df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \" \").csv('HMP_Dataset/'+category+'/'+data_file,schema=schema)\n",
    "        \n",
    "        #create a column called \"source\" storing the current CSV file\n",
    "        temp_df = temp_df.withColumn(\"source\", lit(data_file))\n",
    "        \n",
    "        #create a column called \"class\" storing the current data folder\n",
    "        temp_df = temp_df.withColumn(\"class\", lit(category))\n",
    "        \n",
    "        #append to existing data frame list\n",
    "        #data_frames = data_frames + [temp_df]\n",
    "                                                                                                             \n",
    "        if df is None:\n",
    "            df = temp_df\n",
    "        else:\n",
    "            df = df.union(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('hmp.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKLearn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ibex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibex.sklearn.preprocessing import StandardScaler\n",
    "from ibex.sklearn.preprocessing import LabelEncoder\n",
    "from ibex.sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from ibex import trans\n",
    "\n",
    "pipeline = (trans(LabelEncoder(), in_cols='class') + \n",
    "    trans(StandardScaler(), in_cols=['x', 'y', 'z']) + \n",
    "    trans(OneHotEncoder(), in_cols=['functiontransformer_0'][0]) + \n",
    "    trans(None, in_cols='source')\n",
    ")\n",
    "\n",
    "df_scaled = pipeline.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------------------+-----------------+-----------+\n",
      "|  z|  x|  y|      class|              source|            label|      class|\n",
      "+---+---+---+-----------+--------------------+-----------------+-----------+\n",
      "| 24| 49| 39|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 38| 54| 42|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 22| 49| 45|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 31| 52| 39|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 34| 54| 41|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 37| 53| 30|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 25| 39| 49|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 32| 49| 12|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 26| 31| 41|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 54| 47| 43|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "|  2| 33| 41|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "|  0| 41| 43|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 13| 39| 38|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 12| 39| 34|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 35| 52| 45|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 23| 49| 40|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 31| 55| 37|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 30| 48| 41|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 20| 49| 38|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "| 44| 55| 34|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|\n",
      "+---+---+---+-----------+--------------------+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#register a corresponding query table\n",
    "df.createOrReplaceTempView('df')\n",
    "df_energy = spark.sql(\"\"\"\n",
    "select sqrt(sum(x*x)+sum(y*y)+sum(z*z)) as label, class from df group by class\n",
    "\"\"\")      \n",
    "df_energy.createOrReplaceTempView('df_energy')\n",
    "df_join = spark.sql('select * from df inner join df_energy on df.class=df_energy.class')\n",
    "df_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n",
    "                                  outputCol=\"features\")\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vectorAssembler, normalizer,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------------------+-----------------+-----------+----------------+--------------------+------------------+\n",
      "|  z|  x|  y|      class|              source|            label|      class|        features|       features_norm|        prediction|\n",
      "+---+---+---+-----------+--------------------+-----------------+-----------+----------------+--------------------+------------------+\n",
      "| 24| 49| 39|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[49.0,39.0,24.0]|[0.4375,0.3482142...| 397.3754999617832|\n",
      "| 38| 54| 42|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[54.0,42.0,38.0]|[0.40298507462686...|386.46797661102596|\n",
      "| 22| 49| 45|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[49.0,45.0,22.0]|[0.42241379310344...|404.66685982211425|\n",
      "| 31| 52| 39|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[52.0,39.0,31.0]|[0.42622950819672...| 390.7518308169358|\n",
      "| 34| 54| 41|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[54.0,41.0,34.0]|[0.41860465116279...|389.82063108566774|\n",
      "| 37| 53| 30|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[53.0,30.0,37.0]|[0.44166666666666...| 376.9041503702546|\n",
      "| 25| 39| 49|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[39.0,49.0,25.0]|[0.34513274336283...|402.44249948388887|\n",
      "| 32| 49| 12|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[49.0,12.0,32.0]|[0.52688172043010...|365.61850700214643|\n",
      "| 26| 31| 41|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[31.0,41.0,26.0]|[0.31632653061224...|392.46365505487006|\n",
      "| 54| 47| 43|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[47.0,43.0,54.0]|[0.32638888888888...|368.70363513852186|\n",
      "|  2| 33| 41|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth| [33.0,41.0,2.0]|[0.43421052631578...|418.26616609809565|\n",
      "|  0| 41| 43|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth| [41.0,43.0,0.0]|[0.48809523809523...|424.11602990234286|\n",
      "| 13| 39| 38|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[39.0,38.0,13.0]|[0.43333333333333...|405.58965352564803|\n",
      "| 12| 39| 34|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[39.0,34.0,12.0]|[0.45882352941176...|  403.188375246418|\n",
      "| 35| 52| 45|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[52.0,45.0,35.0]|[0.39393939393939...|391.71843220614795|\n",
      "| 23| 49| 40|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[49.0,40.0,23.0]|[0.4375,0.3571428...|399.29347754640713|\n",
      "| 31| 55| 37|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[55.0,37.0,31.0]|[0.44715447154471...|389.77934420951897|\n",
      "| 30| 48| 41|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[48.0,41.0,30.0]|[0.40336134453781...|392.52670525683095|\n",
      "| 20| 49| 38|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[49.0,38.0,20.0]|[0.45794392523364...|400.72815443642503|\n",
      "| 44| 55| 34|Brush_teeth|Accelerometer-201...|385.8924720696168|Brush_teeth|[55.0,34.0,44.0]|[0.41353383458646...|373.48414733711553|\n",
      "+---+---+---+-----------+--------------------+-----------------+-----------+----------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02054481784295581"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages[2].summary.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression with split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = df.randomSplit([0.8, 0.2])\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "normalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"features_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "binEval.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(df_test)\n",
    "binEval.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SupportVector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "encoder = OneHotEncoder(inputCol=\"label\", outputCol=\"labelVec\")\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n",
    "                                  outputCol=\"features\")\n",
    "normalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"features_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer,lsvc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')\n",
    "df_two_class = spark.sql(\"select * from df where class in ('Use_telephone','Standup_chair')\")\n",
    "\n",
    "splits = df_two_class.randomSplit([0.8, 0.2])\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(df_test)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
